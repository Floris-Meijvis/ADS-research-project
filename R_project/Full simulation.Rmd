---
title: "Data comparison markdown"
author: "Floris Meijvis en co"
date: "2025-03-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is the begin of the document

```{r}
# Load necessary libraries and set a seed
library(nnet) #for multinomial logistic regression
library(ggplot2) #for plots
library(tidyverse)  #for pipe operators
library(tidyr)
library(stats)
library(dplyr)
library(nloptr) #for nonlinear optimisation problems

set.seed(1234) # For reproducibility
``` 

```{r}
#sets up Sim_Results file headers
write.table( t(c("N", "k", "p", "BiasMtoM", "BiasMtoA", "BiasAtoM", "BiasAtoA", "stdMtoM", "stdMtoA", "stdAtoM", "stdAtoA")), file = "Sim_Results", append = FALSE, row.names = F, col.names = F)
```


```{r}
#functions to use for data generation

# Function to generate dataset according to multinom model
# function is set up such that data is ordinal.
generate_ordinal_multinomial_data <- function(N, k, p = 2) {
  # N number of datapoints
  # k number of classes
  # p number of predictors
  
  # Generate predictor values X in [-1,1]
  X = matrix(runif(N * p, -1, 1), ncol = p)
  #X = matrix(sample(c(0,1), N, replace = TRUE), ncol = 1)
  
  # Generate random coefficients for the multinomial logit model
  beta = matrix(rnorm((k - 1) * p, mean = 0, sd = 1), ncol = p)  # (k-1) x p coefficient matrix
  beta_signs = sample(c(-1, 1), p, replace = TRUE) # Assign sign per predictor
  
  # Ensure same sign for each predictor across classes using a for-loop
  for (j in 1:p) {
    beta[, j] = beta_signs[j] * abs(beta[, j])
  }
  
  # Generate intercept coefficients (alpha)
  alpha = rnorm(k - 1) # Intercepts for (k-1) classes
  
  # Compute linear predictors for each class (excluding reference class)
  eta = X %*% t(beta) + matrix(alpha, nrow = N, ncol = k - 1, byrow = TRUE) # N x (k-1)
  
  # Convert to probabilities using softmax transformation
  exp_eta = exp(cbind(0, eta)) # Add reference class with zero logit
  probs = exp_eta / rowSums(exp_eta) # Normalize
  
  # Sample class labels
  y = apply(probs, 1, function(prob) sample(0:(k-1), 1, prob = prob))
  
  # Return dataset as a dataframe along with coefficients
  data = data.frame(Y = as.factor(y), X)
  colnames(data) = c("Y", paste("X", 1:p, sep = ""))
  return(list(data = data, beta = beta, alpha = alpha))#(list(X,probs,y))
}

#function for generating data according to adjacent categories data
generate_adjacent_categories_data = function(N, k, p = 2) {
  # Generate predictor matrix X
  X = matrix(runif(N * p, -1, 1), nrow = N, ncol = p)
  
  # Generate coefficients for each predictor (only one set of betas)
  alpha = sort(runif(k - 1, -1, 1))  # Intercepts
  beta = rnorm(p, mean = 0, sd = 1)  # Single set of regression coefficients for all categories
  
  # Compute cumulative log-odds
  log_odds = matrix(NA, nrow = N, ncol = k - 1)
  for (j in 1:(k - 1)) {
    log_odds[, j] = alpha[j] + j * X %*% beta
  }
  
  # Convert log odds to probabilities
  probs = exp(log_odds)
  probs = cbind(1, probs)  # Include baseline category (category 1)
  probs = probs / rowSums(probs)  # Normalize to sum to 1
  
  # Generate categorical outcomes by sampling based on probabilities
  Y = apply(probs, 1, function(p) sample(0:(k-1), size = 1, prob = p))
  
  # Return as a data frame
  data = data.frame(Y = as.factor(Y), X)
  colnames(data) = c("Y", paste("X", 1:p, sep = ""))
  
  return(list(data = data, beta = beta, alpha = alpha))
}
```

```{r}
#likelihood function and gradients
logLikMultinom = function(par, data, k, p) {
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = k - 1  # Excluding reference class
  num_predictors = p
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = matrix(par[(num_classes + 1):length(par)], nrow = num_classes, ncol = num_predictors)
  
  #ensures same sign for predictors
  #signs = sign(as.numeric(coefs[1, , drop = FALSE]))  # Get sign of first row
  #for (i in 1:ncol(coefs)){
  #  coefs[,i] = abs(coefs[,i]) * signs[i]
  #}
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  # Compute the linear predictors for all classes
  eta = sweep(X %*% t(coefs), 2, intercepts, "+")
  
  # Compute the sum of exponentials for the denominator
  sumExp = rowSums(exp(eta)) + 1  # adding 1 for reference class Y=0
  
  loglik = numeric(length(Y))  #Initialize log-likelihood vector
  
  # Compute log-likelihood per observation
  for (i in seq_along(Y)) {
    if (Y[i] == 0) {
      loglik[i] <- -log(sumExp[i])  # Reference class
    } else {
      loglik[i] <- eta[i, Y[i]] - log(sumExp[i])
    }
  }
  
  return(-sum(loglik))  # Negative log-likelihood for minimization
}

LogLikMultinomGrad = function(par, data){
  #calculates the gradient wrt parameters for the LoglikMultinom function
  
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = length(unique(data$Y)) - 1  # Excluding reference class
  num_predictors = ncol(data) - 1  # Excluding response variable
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = matrix(par[(num_classes + 1):length(par)], nrow = num_classes, ncol = num_predictors)
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  # Compute the nominator of probability for all non-reference class
  nomprob = exp(sweep(X %*% t(coefs), 2, intercepts, "+"))
  
  # Compute the denominator of probabilities
  denomprob = rowSums(nomprob) + 1  # adding 1 for reference class Y=0
  
  ###subroutine for computing gradient wrt intercepts
  #compute number of observations in each class
  class_count = data %>% count(Y) %>% select(n)
  class_count = class_count[-1,] #delete reference class count
  
  #compute sum of probabilities corresponding to classes
  probs = numeric(num_classes)
  for (i in 1:(num_classes)) {
      probs[i] = sum(nomprob[,i]/denomprob)
  }
  
  grad_intercept = class_count - probs
  
  ###subroutine for computing gradient wrt coeficients
  grad_coefs = numeric((num_classes)*num_predictors)
  for (i in 1:num_classes) {
    for (j in 1:num_predictors) {
      grad_coefs[j + (i-1)*num_predictors] = sum(X[,j]) - sum(X[,j] * nomprob[,i]/denomprob)
    }
  }
  
  return(c(grad_intercept, grad_coefs))
}

#example use
#params_multinom = c(data_multinom$alpha, data_multinom$beta)
#logLikMultinomtest = logLikMultinom(params_multinom, data_multinom$data)

logLikAdjacent = function(par, data, k, p) {
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = k - 1  # Excluding reference class
  num_predictors = p
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = par[(num_classes + 1):length(par)]
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  D = matrix(,nrow=length(Y),ncol=num_classes)
  for (i in 1:length(Y)){for (j in 1:num_classes){if (Y[i]==j){D[i,j]=1} else {D[i,j]=0}}}

  # Compute the linear predictors for all classes
  eta = Y * X %*% coefs + D %*% intercepts 
  ETA = matrix(rep(1:(k-1),length(Y)),nrow=length(Y),byrow=T) * c(X %*% coefs) + matrix(rep(intercepts,length(Y)),nrow=length(Y),byrow=T) 

  sumExp = rowSums(exp(ETA)) + 1
  
  loglik = numeric(length(Y))  #Initialize log-likelihood vector
  
  # Compute log-likelihood per observation
  for (i in seq_along(Y)) { loglik[i]<-eta[i]-log(sumExp[i]) 
  }
  
  return(-sum(loglik))  # Negative log-likelihood for minimization
}

#example use
#params_adjacent = c(data_adjacent$alpha, data_adjacent$beta)
#logLikAdjacenttest = logLikAdjacent(params_adjacent, data_adjacent$data)

LogLikAdjacentGrad = function(par, data){
  #calculates the gradient wrt parameters for the LoglikMultinom function
  
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = length(unique(data$Y)) - 1  # Excluding reference class
  num_predictors = ncol(data) - 1  # Excluding response variable
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = par[(num_classes + 1):length(par)]
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  D = matrix(,nrow=length(Y),ncol=num_classes)
  for (i in 1:length(Y)){for (j in 1:num_classes){if (Y[i]==j){D[i,j]=1} else {D[i,j]=0}}}

  # Compute the linear predictors for all classes (except reference class)
  eta = Y * X %*% coefs + D %*% intercepts 
  ETA = matrix(rep(1:(k-1),length(Y)),nrow=length(Y),byrow=T) * c(X %*% coefs) + matrix(rep(intercepts,length(Y)),nrow=length(Y),byrow=T) 

  sumExp = rowSums(exp(ETA)) + 1
  
  ###subroutine for computing gradient wrt intercepts
  #compute number of observations in each class
  class_count = data %>% count(Y) %>% select(n)
  class_count = class_count[-1,] #delete reference class count
  
  probs = numeric(num_classes)
    for (i in 1:num_classes){
      probs[i] = sum(exp(ETA[,i])/sumExp)
    }
  grad_intercept = class_count - probs
  
  ###subroutine for computing gradient wrt coeficients
  grad_coefs = numeric(num_predictors)
  
  #help computes the second term per class
  help = numeric(num_classes)
  for (i in 1:num_predictors) {
      for (j in num_classes){
      help[j] = sum(j * X[,i] * exp(ETA[,j])/sumExp)
    }
    grad_coefs[i] = sum(Y*X[,i]) - sum(help)#THIS TERM IS A WIP.
  }
  
  return(c(grad_intercept, grad_coefs))
}

Ineq_enforcer_multinom = function(par, data, k, p) {
  #function to provide to nloptr routine. ensures same sign constraint
  coefs = matrix(par[k:length(par)], 
                         nrow = k-1, 
                         ncol = p)
  output = c()
  for (i in 1:ncol(coefs)) {
    output[i] = max(coefs[,i]) * min(coefs[,i])
  }
  return(output)
}
```

```{r}
#setting up parameters and dataframe

#parameters to simulate over
k = c(2,3, 4, 5)   #classes
p = c(1,2,3)  #predictors
N = c(30, 50, 100, 250, 500, 1000, 2500)  #datapoints #c(30, 50, 100, 250, 500, 1000, 2500, 5000, 10000)
Nsim = 100 #number of simulations to average over

DataFrame = expand.grid(N = N, k = k, p = p)

#simulation functions for apply()
SimulateAVGBias = function(row) {
  k = row["k"]
  p = row["p"]
  N = row["N"]
  
  #helps us keep track of where we are in the simulation
  print(c(k, p, N))
  
  helpMtoM = rep(0, length = Nsim)
  helpMtoA = rep(0, length = Nsim)
  helpAtoM = rep(0, length = Nsim)
  helpAtoA = rep(0, length = Nsim)
  for (j in 1:Nsim) {#this subroutine performs Nsims for parameters in each row of the DataFrame
    data_multinom = generate_ordinal_multinomial_data(N, k, p)
    data_adjacent = generate_adjacent_categories_data(N, k, p)
    
    ###fitting multinom to multinom
    initial_params = c(data_multinom$alpha, as.vector(data_multinom$beta))

    obj_fn = function(params) logLikMultinom(params, data_multinom$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, data_multinom$data, k, p)

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn,
      eval_g_ineq = constr_fn,
      opts = list("algorithm"="NLOPT_LN_COBYLA",
       "xtol_rel"=1.0e-8),
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                                nrow = k-1, 
                                ncol = p)
    #calculating average bias
    errorcoefs = abs(data_multinom$beta - optimized_coefs)
    errorintercept = abs(data_multinom$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpMtoM[j] = mean(error)
    
    ###fitting multinom to Adjacent
    initial_params = c(data_adjacent$alpha, as.vector(data_adjacent$beta))

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = logLikAdjacent,           # Function to minimize
      #gr = LogLikAdjacentGrad,       # gradient of Function
      k = k,
      p = p,
      data = data_multinom$data,     # Pass the dataset separately
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                                nrow = k-1, 
                                ncol = p)
    #calculating average bias
    errorcoefs = abs(data_adjacent$beta - optimized_coefs)
    errorintercept = abs(data_adjacent$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpMtoA[j] = mean(error)
    
    ###fitting Adjacent to Multinom
    initial_params = c(data_multinom$alpha, as.vector(data_multinom$beta))

    obj_fn = function(params) logLikMultinom(params, data_adjacent$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, data_adjacent$data, k, p)

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn,
      eval_g_ineq = constr_fn,
      opts = list("algorithm"="NLOPT_LN_COBYLA",
       "xtol_rel"=1.0e-8),
    )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                                nrow = k-1, 
                                ncol = p)
    #calculating average bias
    errorcoefs = abs(data_multinom$beta - optimized_coefs)
    errorintercept = abs(data_multinom$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpAtoM[j] = mean(error)
    
    ###fitting Adjacent to Adjacent
    initial_params = c(data_adjacent$alpha, as.vector(data_adjacent$beta))

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = logLikAdjacent,           # Function to minimize
      #gr = LogLikAdjacentGrad,       # gradient of Function
      k = k,
      p = p,
      data = data_adjacent$data,     # Pass the dataset separately
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                                nrow = k-1, 
                                ncol = p)
    #calculating average bias
    errorcoefs = abs(data_adjacent$beta - optimized_coefs)
    errorintercept = abs(data_adjacent$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpAtoA[j] = mean(error)
  }
  output = c(mean(helpMtoM), 
             mean(helpMtoA), 
             mean(helpAtoM), 
             mean(helpAtoA), 
             sqrt(var(helpMtoM)), 
             sqrt(var(helpMtoA)), 
             sqrt(var(helpAtoM)), 
             sqrt(var(helpAtoA)))
    return(output)
}

test = apply(DataFrame, 1, SimulateAVGBias)

test = as.data.frame(t(test))
colnames(test) = c("BiasMtoM", "BiasMtoA", "BiasAtoM", "BiasAtoA", "stdMtoM", "stdMtoA", "stdAtoM", "stdAtoA")

# Add new columns to the original data frame
DataFrame = cbind(DataFrame, test)

write.table(DataFrame, file = "Sim_Results", append = TRUE, row.names = F, col.names = F) #append set to true to allow us to "chop up" the parameter space we want to test into several runs
```

```{r}
#cel for plots
Data = read.table("Sim_Results", header = TRUE)

Data%>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasMtoM, color = 'BiasMtoM')) + 
  geom_point(aes(y = BiasAtoM, color = 'BiasAtoM')) + 
  geom_errorbar(aes(ymin = BiasMtoM - stdMtoM, ymax = BiasMtoM + stdMtoM, color = 'BiasMtoM')) + 
  geom_errorbar(aes(ymin = BiasAtoM - stdAtoM, ymax = BiasAtoM + stdAtoM, color = 'BiasAtoM')) + 
  facet_grid(rows = vars(k), cols = vars(p)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1))
  
Data %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasMtoA, color = 'BiasMtoA')) + 
  geom_point(aes(y = BiasAtoA, color = 'BiasAtoA')) + 
  geom_errorbar(aes(ymin = BiasMtoA - stdMtoA, ymax = BiasMtoA + stdMtoA, color = 'BiasMtoA')) + 
  geom_errorbar(aes(ymin = BiasAtoA - stdAtoA, ymax = BiasAtoA + stdAtoA, color = 'BiasAtoA')) + 
  facet_grid(rows = vars(k), cols = vars(p)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1))
```



Maybe consider using the constrOptim function from the stats library to put the constraints inside of the optimization procedure, and not inside of the likelihood.

sample coefficients outside of the generate_data functions and see if this affects anything!

Gradient functions appear to not work properly, leave them out for now.

- compute and plot expected values of bias in the above plots



