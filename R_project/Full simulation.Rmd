---
title: "Data comparison markdown"
author: "Floris Meijvis en co"
date: "2025-03-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is the begin of the document

```{r}
# Load necessary libraries and set a seet
library(nnet) #for multinomial logistic regression
library(ggplot2) #for plots
library(tidyverse)  #for pipe operators
library(tidyr)
library(stats)
library(nloptr) #for nonlinear optimisation problems
library(dplyr)
library(numDeriv) #for numeric derivatives

set.seed(1234) # For reproducibility
``` 

```{r}
#sets up Sim_Results file headers
write.table( t(c("N", "k", "p", "BiasMtoM", "BiasMtoA", "BiasAtoM", "BiasAtoA", "stdMtoM", "stdMtoA", "stdAtoM", "stdAtoA")), file = "Sim_Results", append = FALSE, row.names = F, col.names = F)
```


```{r}
#functions to use for data generation

# Function to generate dataset according to multinom model
# function is set up such that data is ordinal.
generate_ordinal_multinomial_data <- function(N, k, p = 2) {
  # N number of datapoints
  # k number of classes
  # p number of predictors
  
  # Generate predictor values X in [-1,1]
  X = matrix(runif(N * p, -1, 1), ncol = p)
  #X = matrix(sample(c(0,1), N, replace = TRUE), ncol = 1)
  
  # Generate random coefficients for the multinomial logit model
  beta = matrix(rnorm((k - 1) * p, mean = 0, sd = 1), ncol = p)  # (k-1) x p coefficient matrix
  beta_signs = sample(c(-1, 1), p, replace = TRUE) # Assign sign per predictor
  
  # Ensure same sign for each predictor across classes using a for-loop
  for (j in 1:p) {
    beta[, j] = beta_signs[j] * abs(beta[, j])
  }
  
  # Generate intercept coefficients (alpha)
  alpha = rnorm(k - 1) # Intercepts for (k-1) classes
  
  # Compute linear predictors for each class (excluding reference class)
  eta = X %*% t(beta) + matrix(alpha, nrow = N, ncol = k - 1, byrow = TRUE) # N x (k-1)
  
  # Convert to probabilities using softmax transformation
  exp_eta = exp(cbind(0, eta)) # Add reference class with zero logit
  probs = exp_eta / rowSums(exp_eta) # Normalize
  
  # Sample class labels
  y = apply(probs, 1, function(prob) sample(0:(k-1), 1, prob = prob))
  
  # Return dataset as a dataframe along with coefficients
  data = data.frame(Y = as.factor(y), X)
  colnames(data) = c("Y", paste("X", 1:p, sep = ""))
  return(list(data = data, beta = beta, alpha = alpha))#(list(X,probs,y))
}

#function for generating data according to adjacent categories data
generate_adjacent_categories_data = function(N, k, p = 2) {
  # Generate predictor matrix X
  X = matrix(runif(N * p, -1, 1), nrow = N, ncol = p)
  
  # Generate coefficients for each predictor (only one set of betas)
  alpha = sort(runif(k - 1, -1, 1))  # Intercepts
  beta = rnorm(p, mean = 0, sd = 1)  # Single set of regression coefficients for all categories
  
  # Compute cumulative log-odds
  log_odds = matrix(NA, nrow = N, ncol = k - 1)
  for (j in 1:(k - 1)) {
    log_odds[, j] = alpha[j] + j * X %*% beta
  }
  
  # Convert log odds to probabilities
  probs = exp(log_odds)
  probs = cbind(1, probs)  # Include baseline category (category 1)
  probs = probs / rowSums(probs)  # Normalize to sum to 1
  
  # Generate categorical outcomes by sampling based on probabilities
  Y = apply(probs, 1, function(p) sample(0:(k-1), size = 1, prob = p))
  
  # Return as a data frame
  data = data.frame(Y = as.factor(Y), X)
  colnames(data) = c("Y", paste("X", 1:p, sep = ""))
  
  return(list(data = data, beta = beta, alpha = alpha))
}
```

```{r}
#likelihood function and gradients
LogLikMultinom = function(par, data, k, p) {
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = k - 1  # Excluding reference class
  num_predictors = p
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = matrix(par[(num_classes + 1):length(par)], nrow = num_classes, ncol = num_predictors)
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  # Compute the linear predictors for all classes
  eta = sweep(X %*% t(coefs), 2, intercepts, "+")
  
  # Compute the sum of exponentials for the denominator
  sumExp = rowSums(exp(eta)) + 1  # adding 1 for reference class Y=0
  
  loglik = numeric(length(Y))  #Initialize log-likelihood vector
  
  # Compute log-likelihood per observation
  for (i in seq_along(Y)) {
    if (Y[i] == 0) {
      loglik[i] <- -log(sumExp[i])  # Reference class
    } else {
      loglik[i] <- eta[i, Y[i]] - log(sumExp[i])
    }
  }
  
  return(-sum(loglik))  # Negative log-likelihood for minimization
}

LogLikMultinomGrad = function(par, data, k, p) {
  
  num_classes = k - 1 #number of non-reference classes
  num_predictors = p
  
  intercepts = par[1:num_classes]
  coefs = matrix(par[(num_classes + 1):length(par)], nrow = num_classes, ncol = num_predictors)
  
  X = as.matrix(data[, -1])
  Y = as.integer(data$Y) - 1
  
  eta = sweep(X %*% t(coefs), 2, intercepts, "+")
  # Compute the sum of exponentials for the denominator
  sumExp = rowSums(exp(eta)) + 1  # adding 1 for reference class Y=0
  
  probs = exp(eta) / sumExp  # N x K matrix
  
  # Indicator matrix for observed Y
  N = nrow(data)
  ind = matrix(0, nrow = N, ncol = num_classes)
  for (i in 1:N) {
    if (Y[i] > 0) {
      ind[i, Y[i]] = 1
    }
  }
  
  diff = ind - probs  # N x K
  
  # Gradient for intercepts
  grad_intercepts = -colSums(diff)
  
  # Gradient for coefficients
  grad_coefs = -t(diff) %*% X  # K x p matrix
  
  # Flatten gradient into vector
  grad = c(grad_intercepts, as.vector(grad_coefs))
  return(grad)
}

LogLikAdjacent = function(par, data, k, p) {
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = k - 1  # Excluding reference class
  num_predictors = p
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = par[(num_classes + 1):length(par)]
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  D = matrix(,nrow=length(Y),ncol=num_classes)
  for (i in 1:length(Y)){for (j in 1:num_classes){if (Y[i]==j){D[i,j]=1} else {D[i,j]=0}}}

  # Compute the linear predictors for all classes
  eta = Y * X %*% coefs + D %*% intercepts 
  ETA = matrix(rep(1:(k-1),length(Y)),nrow=length(Y),byrow=T) * c(X %*% coefs) + matrix(rep(intercepts,length(Y)),nrow=length(Y),byrow=T) 

  sumExp = rowSums(exp(ETA)) + 1
  
  loglik = numeric(length(Y))  #Initialize log-likelihood vector
  
  # Compute log-likelihood per observation
  for (i in seq_along(Y)) { loglik[i]<-eta[i]-log(sumExp[i]) 
  }
  
  return(-sum(loglik))  # Negative log-likelihood for minimization
}

LogLikAdjacentGrad <- function(par, data, k, p) {
  # Unpack dimensions
  num_classes <- k - 1
  num_predictors <- p
  
  # Reconstruct parameters
  intercepts <- par[1:num_classes]
  coefs <- par[(num_classes + 1):length(par)]
  
  # Prepare data
  X <- as.matrix(data[, -1])
  Y <- as.integer(data$Y) - 1
  n <- nrow(X)
  
  # Compute ETA matrix
  linear_part <- X %*% coefs
  ETA <- matrix(0, n, num_classes)
  for (j in 1:num_classes) {
    ETA[, j] <- intercepts[j] + j * linear_part
  }
  
  expETA <- exp(ETA)
  denom <- rowSums(expETA) + 1
  P <- expETA / denom  # n x num_classes
  
  # Gradient for intercepts
  grad_alpha <- numeric(num_classes)
  for (j in 1:num_classes) {
    grad_alpha[j] <- sum((Y == j) - P[, j])
  }
  
  # Gradient for coefficients
  grad_beta <- numeric(num_predictors)
  for (l in 1:num_predictors) {
    x_l <- X[, l]
    weighted_sum <- rowSums(sweep(P, 2, 1:num_classes, FUN = "*")) * x_l
    grad_beta[l] <- sum(Y * x_l - weighted_sum)
  }
  
  return(-c(grad_alpha, grad_beta))  # Negative gradient for minimization
}


Ineq_enforcer_multinom = function(par, data, k, p) {
  #function to provide to nloptr routine. ensures same sign constraint
  coefs = matrix(par[k:length(par)], 
                         nrow = k-1, 
                         ncol = p)
  output = c()
  for (i in 1:ncol(coefs)) {
    output[i] = -1 * max(coefs[,i]) * min(coefs[,i]) #for nloptr need to remain in \leq 0 region
  }
  return(output)
}
```

```{r}
#setting up parameters and dataframe

#parameters to simulate over
k = c(2,3, 4, 5)   #classes
p = c(1,2,3)  #predictors
N = c(30, 50, 100, 250, 500, 1000, 2500)  #datapoints #c(30, 50, 100, 250, 500, 1000, 2500, 5000, 10000)
Nsim = 100 #number of simulations to average over

DataFrame = expand.grid(N = N, k = k, p = p)

#simulation functions for apply()
SimulateAVGBias = function(row) {
  k = row["k"]
  p = row["p"]
  N = row["N"]
  
  #helps us keep track of where we are in the simulation
  print(c(k, p, N))
  
  helpMtoM = rep(0, length = Nsim)
  helpMtoA = rep(0, length = Nsim)
  helpAtoM = rep(0, length = Nsim)
  helpAtoA = rep(0, length = Nsim)
  for (j in 1:Nsim) {#this subroutine performs Nsims for parameters in each row of the DataFrame
    data_multinom = generate_ordinal_multinomial_data(N, k, p)
    data_adjacent = generate_adjacent_categories_data(N, k, p)
    
    ###fitting multinom to multinom
    initial_params = c(rep(data_multinom$alpha),  # Intercepts
                   as.vector(data_multinom$beta))   # Coefficients
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    #wrapper functions
    obj_fn = function(params) LogLikMultinom(params, data_multinom$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, data_multinom$data, k, p)
    grad_obj_fn = function(params) LogLikMultinomGrad(params, data_multinom$data, k, p)
    grad_constr_fn = function(par) {jacobian(constr_fn, par)}

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn, #objective function
      eval_grad_f = grad_obj_fn, #gradient of objective function
      eval_g_ineq = constr_fn, # constraint
      eval_jac_g_ineq = grad_constr_fn, #gradient of constraint
      opts = list("algorithm"="NLOPT_LD_MMA",
                  "xtol_rel"=1.0e-8,
                  "maxeval" = 1000)
                  )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                         nrow = k-1, 
                         ncol = p)
    
    #calculating average bias
    errorcoefs = abs(data_multinom$beta - optimized_coefs)
    errorintercept = abs(data_multinom$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpMtoM[j] = mean(error)
    
    ###fitting multinom to Adjacent
    initial_params = c(data_adjacent$alpha, as.vector(data_adjacent$beta))

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = LogLikAdjacent,           # Function to minimize
      gr = LogLikAdjacentGrad,       # gradient of Function
      data = data_multinom$data,     # Pass the dataset separately
      k = k,
      p = p,
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                                nrow = k-1, 
                                ncol = p)
    #calculating average bias
    errorcoefs = abs(data_adjacent$beta - optimized_coefs)
    errorintercept = abs(data_adjacent$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpMtoA[j] = mean(error)
    
    ###fitting Adjacent to Multinom
    initial_params = c(rep(data_multinom$alpha),  # Intercepts
                   as.vector(data_multinom$beta))   # Coefficients
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    obj_fn = function(params) LogLikMultinom(params, data_adjacent$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, data_adjacent$data, k, p)
    grad_obj_fn = function(params) LogLikMultinomGrad(params, data_adjacent$data, k, p)
    grad_constr_fn = function(par) {jacobian(constr_fn, par)}

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn, #objective function
      eval_grad_f = grad_obj_fn, #gradient of objective function
      eval_g_ineq = constr_fn, # constraint
      eval_jac_g_ineq = grad_constr_fn, #gradient of constraint
      opts = list("algorithm"="NLOPT_LD_MMA",
                  "xtol_rel"=1.0e-8,
                  "maxeval" = 1000)
                  )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                         nrow = k-1, 
                         ncol = p)
    
    #calculating average bias
    errorcoefs = abs(data_multinom$beta - optimized_coefs)
    errorintercept = abs(data_multinom$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpAtoM[j] = mean(error)
    
    ###fitting Adjacent to Adjacent
    initial_params = c(data_adjacent$alpha, as.vector(data_adjacent$beta))

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = LogLikAdjacent,           # Function to minimize
      gr = LogLikAdjacentGrad,       # gradient of Function
      data = data_adjacent$data,     # Pass the dataset separately
      k = k,
      p = p,
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                                nrow = k-1, 
                                ncol = p)
    #calculating average bias
    errorcoefs = abs(data_adjacent$beta - optimized_coefs)
    errorintercept = abs(data_adjacent$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpAtoA[j] = mean(error)
  }
  output = c(mean(helpMtoM), 
             mean(helpMtoA), 
             mean(helpAtoM), 
             mean(helpAtoA), 
             sqrt(var(helpMtoM)), 
             sqrt(var(helpMtoA)), 
             sqrt(var(helpAtoM)), 
             sqrt(var(helpAtoA)))
    return(output)
}

test = apply(DataFrame, 1, SimulateAVGBias)

test = as.data.frame(t(test))
colnames(test) = c("BiasMtoM", "BiasMtoA", "BiasAtoM", "BiasAtoA", "stdMtoM", "stdMtoA", "stdAtoM", "stdAtoA")

# Add new columns to the original data frame
DataFrame = cbind(DataFrame, test)

write.table(DataFrame, file = "Sim_Results", append = TRUE, row.names = F, col.names = F) #append set to true to allow us to "chop up" the parameter space we want to test into several runs
```

```{r}
#cel for plots
Data = read.table("Sim_Results", header = TRUE)

Data %>% filter(N > 30) %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasMtoM, color = 'BiasMtoM')) + 
  geom_point(aes(y = BiasAtoM, color = 'BiasAtoM')) + 
  geom_errorbar(aes(ymin = BiasMtoM - stdMtoM, ymax = BiasMtoM + stdMtoM, color = 'BiasMtoM')) + 
  geom_errorbar(aes(ymin = BiasAtoM - stdAtoM, ymax = BiasAtoM + stdAtoM, color = 'BiasAtoM')) + 
  facet_grid(rows = vars(k), cols = vars(p)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1))
  
Data %>% filter(N > 30) %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasMtoA, color = 'BiasMtoA')) + 
  geom_point(aes(y = BiasAtoA, color = 'BiasAtoA')) + 
  geom_errorbar(aes(ymin = BiasMtoA - stdMtoA, ymax = BiasMtoA + stdMtoA, color = 'BiasMtoA')) + 
  geom_errorbar(aes(ymin = BiasAtoA - stdAtoA, ymax = BiasAtoA + stdAtoA, color = 'BiasAtoA')) + 
  facet_grid(rows = vars(k), cols = vars(p)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1))
```


sample coefficients outside of the generate_data functions and see if this affects anything!

Gradient functions appear to not work properly, leave them out for now.

- compute and plot expected values of bias in the above plots

- compute variance and std of every parameter individually. gives an idea of efficiency

- investigate why std's sometimes increase as dataset grows larger



