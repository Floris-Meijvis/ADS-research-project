---
title: "Data comparison markdown"
author: "Floris Meijvis en co"
date: "2025-03-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is the begin of the document

```{r}
# Load necessary libraries and set a seed
library(nnet) #for multinomial logistic regression
library(ggplot2) #for plots
library(tidyverse)  #for pipe operators
library(tidyr)
library(stats)
library(nloptr) #for nonlinear optimisation problems
library(dplyr)
library(numDeriv) #for numeric derivatives
library(caret)

seed = 1234 # For reproducibility, set.seed is called inside of the simulation functions
``` 

```{r}
#sets up Sim_Results file headers
write.table( t(c("N", "k", "p", "BiasMtoM", "BiasMtoA", "BiasAtoM", "BiasAtoA", "stdMtoM", "stdMtoA", "stdAtoM", "stdAtoA")), file = "Sim_ResultsV2", append = FALSE, row.names = F, col.names = F)
```


```{r}
#functions to use for data generation

N = 2500  
p = 3
k = 5

# Generate parameters for O.M. model
beta_M = matrix(rnorm((k - 1) * p, mean = 0, sd = 1), ncol = p)  # (k-1) x p coefficient matrix
beta_signs = sample(c(-1, 1), p, replace = TRUE) # Assign sign per predictor
  
# Ensure same sign for each predictor across classes using a for-loop
for (j in 1:p) {
  beta_M[, j] = beta_signs[j] * abs(beta_M[, j])
  }
  
# Generate intercept coefficients (alpha)
alpha_M = rnorm(k - 1) # Intercepts for (k-1) classes
  
generate_ordinal_multinomial_data = function(N, k, p, alpha, beta) {
  # N number of datapoints
  # k number of classes
  # p number of predictors
  # alpha intercepts
  # beta coefficient matrix
  
  # Generate predictor values X in [-1,1]
  X = matrix(runif(N * p, -2, 2), ncol = p)
  #X = matrix(sample(c(0,1), N, replace = TRUE), ncol = 1)

  # Compute linear predictors for each class (excluding reference class)
  eta = X %*% t(beta) + matrix(alpha, nrow = N, ncol = k - 1, byrow = TRUE) # N x (k-1)
  
  # Convert to probabilities using softmax transformation
  exp_eta = exp(cbind(0, eta)) # Add reference class with zero logit
  probs = exp_eta / rowSums(exp_eta) # Normalize
  
  # Sample class labels
  y = apply(probs, 1, function(prob) sample(0:(k-1), 1, prob = prob))
  
  # Return dataset as a dataframe along with coefficients
  data = data.frame(Y = as.factor(y), X)
  colnames(data) = c("Y", paste("X", 1:p, sep = ""))
  return(list(data = data, beta = beta, alpha = alpha))#(list(X,probs,y))
}

# generate parameters of A.C. model
alpha_A = sort(runif(k - 1, -1, 1))  # Intercepts
beta_A = rnorm(p, mean = 0, sd = 1)  # Single set of regression coefficients for all categories
  
#function for generating data according to adjacent categories data
generate_adjacent_categories_data = function(N, k, p, alpha, beta) {
  # Generate predictor matrix X
  X = matrix(runif(N * p, -2, 2), nrow = N, ncol = p)
  
  # Compute cumulative log-odds
  log_odds = matrix(NA, nrow = N, ncol = k - 1)
  for (j in 1:(k - 1)) {
    log_odds[, j] = alpha[j] + j * X %*% beta
  }
  
  # Convert log odds to probabilities
  probs = exp(log_odds)
  probs = cbind(1, probs)  # Include baseline category (category 1)
  probs = probs / rowSums(probs)  # Normalize to sum to 1
  
  # Generate categorical outcomes by sampling based on probabilities
  Y = apply(probs, 1, function(p) sample(0:(k-1), size = 1, prob = p))
  
  # Return as a data frame
  data = data.frame(Y = as.factor(Y), X)
  colnames(data) = c("Y", paste("X", 1:p, sep = ""))
  
  return(list(data = data, beta = beta, alpha = alpha))
}
```

```{r}
#likelihood function and gradients
LogLikMultinom = function(par, data, k, p) {
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = k - 1  # Excluding reference class
  num_predictors = p
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = matrix(par[(num_classes + 1):length(par)], nrow = num_classes, ncol = num_predictors)
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  # Compute the linear predictors for all classes
  eta = sweep(X %*% t(coefs), 2, intercepts, "+")
  
  # Compute the sum of exponentials for the denominator
  sumExp = rowSums(exp(eta)) + 1  # adding 1 for reference class Y=0
  
  loglik = numeric(length(Y))  #Initialize log-likelihood vector
  
  # Compute log-likelihood per observation
  for (i in seq_along(Y)) {
    if (Y[i] == 0) {
      loglik[i] = -log(sumExp[i])  # Reference class
    } else {
      loglik[i] = eta[i, Y[i]] - log(sumExp[i])
    }
  }
  
  return(-sum(loglik))  # Negative log-likelihood for minimization
}

LogLikMultinomGrad = function(par, data, k, p) {
  
  num_classes = k - 1 #number of non-reference classes
  num_predictors = p
  
  intercepts = par[1:num_classes]
  coefs = matrix(par[(num_classes + 1):length(par)], nrow = num_classes, ncol = num_predictors)
  
  X = as.matrix(data[, -1])
  Y = as.integer(data$Y) - 1
  
  eta = sweep(X %*% t(coefs), 2, intercepts, "+")
  # Compute the sum of exponentials for the denominator
  sumExp = rowSums(exp(eta)) + 1  # adding 1 for reference class Y=0
  
  probs = exp(eta) / sumExp  # N x K matrix
  
  # Indicator matrix for observed Y
  N = nrow(data)
  ind = matrix(0, nrow = N, ncol = num_classes)
  for (i in 1:N) {
    if (Y[i] > 0) {
      ind[i, Y[i]] = 1
    }
  }
  
  diff = ind - probs  # N x K
  
  # Gradient for intercepts
  grad_intercepts = -colSums(diff)
  
  # Gradient for coefficients
  grad_coefs = -t(diff) %*% X  # K x p matrix
  
  # Flatten gradient into vector
  grad = c(grad_intercepts, as.vector(grad_coefs))
  return(grad)
}

LogLikAdjacent = function(par, data, k, p) {
  #INPUT:
  #par: flattened parameter matrix, see example below for correct creation
  #data: dataset to calculate likelihood for, first column should equal class and be called Y
  
  # Extract dimensions
  num_classes = k - 1  # Excluding reference class
  num_predictors = p
  
  # Reconstruct matrix parameters from par vector
  intercepts = par[1:num_classes]  # First few elements are intercepts
  coefs = par[(num_classes + 1):length(par)]
  
  # Extract variables
  X = as.matrix(data[, -1])  # Convert to matrix
  Y = as.integer(data$Y) - 1  # Convert factor to integer (-1 to align indices)
  
  D = matrix(,nrow=length(Y),ncol=num_classes)
  for (i in 1:length(Y)){for (j in 1:num_classes){if (Y[i]==j){D[i,j]=1} else {D[i,j]=0}}}

  # Compute the linear predictors for all classes
  eta = Y * X %*% coefs + D %*% intercepts 
  ETA = matrix(rep(1:(k-1),length(Y)),nrow=length(Y),byrow=T) * c(X %*% coefs) + matrix(rep(intercepts,length(Y)),nrow=length(Y),byrow=T) 

  sumExp = rowSums(exp(ETA)) + 1
  
  loglik = numeric(length(Y))  #Initialize log-likelihood vector
  
  # Compute log-likelihood per observation
  for (i in seq_along(Y)) { loglik[i] = eta[i]-log(sumExp[i]) 
  }
  
  return(-sum(loglik))  # Negative log-likelihood for minimization
}

LogLikAdjacentGrad = function(par, data, k, p) {
  # Unpack dimensions
  num_classes = k - 1
  num_predictors = p
  
  # Reconstruct parameters
  intercepts = par[1:num_classes]
  coefs = par[(num_classes + 1):length(par)]
  
  # Prepare data
  X = as.matrix(data[, -1])
  Y = as.integer(data$Y) - 1
  n = nrow(X)
  
  # Compute ETA matrix
  linear_part = X %*% coefs
  ETA = matrix(0, n, num_classes)
  for (j in 1:num_classes) {
    ETA[, j] = intercepts[j] + j * linear_part
  }
  
  expETA = exp(ETA)
  denom = rowSums(expETA) + 1
  P = expETA / denom  # n x num_classes
  
  # Gradient for intercepts
  grad_alpha = numeric(num_classes)
  for (j in 1:num_classes) {
    grad_alpha[j] = sum((Y == j) - P[, j])
  }
  
  # Gradient for coefficients
  grad_beta = numeric(num_predictors)
  for (l in 1:num_predictors) {
    x_l = X[, l]
    weighted_sum = rowSums(sweep(P, 2, 1:num_classes, FUN = "*")) * x_l
    grad_beta[l] = sum(Y * x_l - weighted_sum)
  }
  
  return(-c(grad_alpha, grad_beta))  # Negative gradient for minimization
}


Ineq_enforcer_multinom = function(par, data, k, p) {
  #function to provide to nloptr routine. ensures same sign constraint
  coefs = matrix(par[k:length(par)], 
                         nrow = k-1, 
                         ncol = p)
  output = c()
  for (i in 1:ncol(coefs)) {
    output[i] = -1 * max(coefs[,i]) * min(coefs[,i]) #for nloptr need to remain in \leq 0 region
  }
  return(output)
}
```

```{r}
#setting up parameters and dataframe

#parameters to simulate over
k = c(2,3, 4, 5)   #classes
p = c(1,2,3)  #predictors
N = c(30, 50, 100, 250, 500)  #datapoints #c(30, 50, 100, 250, 500, 1000, 2500, 5000, 10000)
Nsim = 100 #number of simulations to average over

DataFrame = expand.grid(N = N, k = k, p = p)

#simulation functions for apply()
SimulateAVGBias = function(row) {
  set.seed(seed)
  k = row["k"]
  p = row["p"]
  N = row["N"]
  
  #helps us keep track of where we are in the simulation
  print(c(k, p, N))
  
  #generates coefficients for data generation
  beta_M = matrix(rnorm((k - 1) * p, mean = 0, sd = 1), ncol = p)
  beta_signs = sample(c(-1, 1), p, replace = TRUE)
  for (j in 1:p) {
    beta_M[, j] = beta_signs[j] * abs(beta_M[, j])
    }
  alpha_M = rnorm(k - 1)
  alpha_A = sort(runif(k - 1, -1, 1)) 
  
  beta_A = rnorm(p, mean = 0, sd = 1)
  
  helpMtoM = rep(0, length = Nsim)
  helpMtoA = rep(0, length = Nsim)
  helpAtoM = rep(0, length = Nsim)
  helpAtoA = rep(0, length = Nsim)
  for (j in 1:Nsim) {#this subroutine performs Nsims for parameters in each row of the DataFrame
    data_multinom = generate_ordinal_multinomial_data(N, k, p, alpha_M, beta_M)
    data_adjacent = generate_adjacent_categories_data(N, k, p, alpha_A, beta_A)
    
    ###fitting multinom to multinom
    initial_params = c(rep(data_multinom$alpha),  # Intercepts
                   as.vector(data_multinom$beta))   # Coefficients
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    #wrapper functions
    obj_fn = function(params) LogLikMultinom(params, data_multinom$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, data_multinom$data, k, p)
    grad_obj_fn = function(params) LogLikMultinomGrad(params, data_multinom$data, k, p)
    grad_constr_fn = function(par) {jacobian(constr_fn, par)}

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn, #objective function
      eval_grad_f = grad_obj_fn, #gradient of objective function
      eval_g_ineq = constr_fn, # constraint
      eval_jac_g_ineq = grad_constr_fn, #gradient of constraint
      opts = list("algorithm"="NLOPT_LD_MMA",
                  "xtol_rel"=1.0e-8,
                  "maxeval" = 1000)
                  )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                         nrow = k-1, 
                         ncol = p)
    
    #calculating average bias
    errorcoefs = abs(data_multinom$beta - optimized_coefs)
    errorintercept = abs(data_multinom$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpMtoM[j] = mean(error)
    
    ###fitting multinom to Adjacent
    initial_params = c(data_adjacent$alpha, as.vector(data_adjacent$beta))
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = LogLikAdjacent,           # Function to minimize
      gr = LogLikAdjacentGrad,       # gradient of Function
      data = data_multinom$data,     # Pass the dataset separately
      k = k,
      p = p,
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = optimized_params[k:length(optimized_params)]
    
    #calculating average bias
    errorcoefs = abs(data_adjacent$beta - optimized_coefs)
    errorintercept = abs(data_adjacent$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpMtoA[j] = mean(error)
    
    ###fitting Adjacent to Multinom
    initial_params = c(rep(data_multinom$alpha),  # Intercepts
                   as.vector(data_multinom$beta))   # Coefficients
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    obj_fn = function(params) LogLikMultinom(params, data_adjacent$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, data_adjacent$data, k, p)
    grad_obj_fn = function(params) LogLikMultinomGrad(params, data_adjacent$data, k, p)
    grad_constr_fn = function(par) {jacobian(constr_fn, par)}

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn, #objective function
      eval_grad_f = grad_obj_fn, #gradient of objective function
      eval_g_ineq = constr_fn, # constraint
      eval_jac_g_ineq = grad_constr_fn, #gradient of constraint
      opts = list("algorithm"="NLOPT_LD_MMA",
                  "xtol_rel"=1.0e-8,
                  "maxeval" = 1000)
                  )

    #Reconstruction of original data shape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                         nrow = k-1, 
                         ncol = p)
    
    #calculating average bias
    errorcoefs = abs(data_multinom$beta - optimized_coefs)
    errorintercept = abs(data_multinom$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpAtoM[j] = mean(error)
    
    ###fitting Adjacent to Adjacent
    initial_params = c(data_adjacent$alpha, as.vector(data_adjacent$beta))
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = LogLikAdjacent,           # Function to minimize
      gr = LogLikAdjacentGrad,       # gradient of Function
      data = data_adjacent$data,     # Pass the dataset separately
      k = k,
      p = p,
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = optimized_params[k:length(optimized_params)]
    
    #calculating average bias
    errorcoefs = abs(data_adjacent$beta - optimized_coefs)
    errorintercept = abs(data_adjacent$alpha - optimized_intercepts)
    error = c(errorintercept, errorcoefs)
    helpAtoA[j] = mean(error)
  }
  output = c(mean(helpMtoM), 
             mean(helpMtoA), 
             mean(helpAtoM), 
             mean(helpAtoA), 
             sqrt(var(helpMtoM)), 
             sqrt(var(helpMtoA)), 
             sqrt(var(helpAtoM)), 
             sqrt(var(helpAtoA)))
    return(output)
}

test = apply(DataFrame, 1, SimulateAVGBias)

test = as.data.frame(t(test))
colnames(test) = c("BiasMtoM", "BiasMtoA", "BiasAtoM", "BiasAtoA", "stdMtoM", "stdMtoA", "stdAtoM", "stdAtoA")

# Add new columns to the original data frame
DataFrame = cbind(DataFrame, test)

write.table(DataFrame, file = "Sim_ResultsV2", append = TRUE, row.names = F, col.names = F) #append set to true to allow us to "chop up" the parameter space we want to test into several runs
```

```{r}
#cel for plots
Data = read.table("Sim_ResultsV2", header = TRUE)

Data %>% #filter(N > 30) %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasMtoM, color = 'Ord. Mult. data')) + 
  geom_point(aes(y = BiasAtoM, color = 'Adj. cat. data')) + 
  #geom_errorbar(aes(ymin = BiasMtoM - stdMtoM, ymax = BiasMtoM + stdMtoM, color = 'Ord. Mult. data')) +
  #geom_errorbar(aes(ymin = BiasAtoM - stdAtoM, ymax = BiasAtoM + stdAtoM, color = 'Adj. cat. data')) + 
  facet_grid(rows = vars(k), cols = vars(p), labeller = labeller(.rows = label_both, .cols = label_both)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1)) + 
  labs(title = "Average Bias across all parameters when fitting to Ordinal Multinomial Model") + 
  ylab("Bias") + 
  xlab("N, # datapoints") + 
  scale_y_log10()
  
Data %>% #filter(N > 30) %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasMtoA, color = 'Ord. Mult. data')) + 
  geom_point(aes(y = BiasAtoA, color = 'Adj. cat. data')) + 
  #geom_errorbar(aes(ymin = BiasMtoA - stdMtoA, ymax = BiasMtoA + stdMtoA, color = 'Ord. Mult. data')) +
  #geom_errorbar(aes(ymin = BiasAtoA - stdAtoA, ymax = BiasAtoA + stdAtoA, color = 'Adj. cat. data')) + 
  facet_grid(rows = vars(k), cols = vars(p), labeller = labeller(.rows = label_both, .cols = label_both)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1)) + 
  labs(title = "Average Bias across all parameters when fitting to Adjacent Categories Model") + 
  ylab("Bias") + 
  xlab("N, # datapoints")+ 
  scale_y_log10()

Data %>% #filter(N > 30) %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasMtoM, color = 'Ord. Mult. model')) + 
  geom_point(aes(y = BiasMtoA, color = 'Adj. cat. model')) + 
  #geom_errorbar(aes(ymin = BiasMtoM - stdMtoM, ymax = BiasMtoM + stdMtoM, color = 'Ord. Mult. model'))+
  #geom_errorbar(aes(ymin = BiasMtoA - stdMtoA, ymax = BiasMtoA + stdMtoA, color = 'Adj. cat. model')) +
  facet_grid(rows = vars(k), cols = vars(p), labeller = labeller(.rows = label_both, .cols = label_both)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1)) + 
  labs(title = "Average Bias across all parameters when fitting Ordinal Multinomial data") + 
  ylab("Bias") + 
  xlab("N, # datapoints")+ 
  scale_y_log10()

Data %>% #filter(N > 30) %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasAtoM, color = 'Ord. Mult. model')) + 
  geom_point(aes(y = BiasAtoA, color = 'Adj. cat. model')) + 
  #geom_errorbar(aes(ymin = BiasAtoM - stdAtoM, ymax = BiasAtoM + stdAtoM, color = 'Ord. Mult. model'))+
  #geom_errorbar(aes(ymin = BiasAtoA - stdAtoA, ymax = BiasAtoA + stdAtoA, color = 'Adj. cat. model')) +
  facet_grid(rows = vars(k), cols = vars(p), labeller = labeller(.rows = label_both, .cols = label_both)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1)) + 
  labs(title = "Average Bias across all parameters when fitting Adjacent categories data") + 
  ylab("Bias") + 
  xlab("N, # datapoints")+ 
  scale_y_log10()

Data %>% #filter(N > 30) %>%
  ggplot(aes(x = N)) + 
  geom_point(aes(y = BiasAtoM, color = 'AtoM')) + 
  geom_point(aes(y = BiasMtoA, color = 'MtoA')) + 
  #geom_errorbar(aes(ymin = BiasAtoM - stdAtoM, ymax = BiasAtoM + stdAtoM, color = 'AtoM'))+
  #geom_errorbar(aes(ymin = BiasMtoA - stdMtoA, ymax = BiasMtoA + stdMtoA, color = 'MtoA')) +
  facet_grid(rows = vars(k), cols = vars(p), labeller = labeller(.rows = label_both, .cols = label_both)) + 
  theme(panel.border = element_rect(color = "black", fill = NA, linewidth = 1)) + 
  labs(title = "model effectiveness comparison") + 
  ylab("Bias") + 
  xlab("N, # datapoints")+ 
  scale_y_log10()
```

```{r}
#WIP, cel for estimating efficiency
simulate_efficiency = function(N,k,p, Nsim) {
  set.seed(seed)
  
  beta_M = matrix(rnorm((k - 1) * p, mean = 0, sd = 1), ncol = p)
  beta_signs = sample(c(-1, 1), p, replace = TRUE)
  for (j in 1:p) {
    beta_M[, j] = beta_signs[j] * abs(beta_M[, j])
    }
  alpha_M = rnorm(k - 1)
  alpha_A = sort(runif(k - 1, -1, 1)) 
  
  beta_A = rnorm(p, mean = 0, sd = 1)
  
  df_MtoM = c() 
  
  for (j in 1:Nsim) {
    data_multinom = generate_ordinal_multinomial_data(N, k, p, alpha_M, beta_M)
    data_adjacent = generate_adjacent_categories_data(N, k, p, alpha_A, beta_A)
    
    ###fitting multinom to multinom
    initial_params = c(rep(data_multinom$alpha),  # Intercepts
                   as.vector(data_multinom$beta))   # Coefficients
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    #wrapper functions
    obj_fn = function(params) LogLikMultinom(params, data_multinom$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, data_multinom$data, k, p)
    grad_obj_fn = function(params) LogLikMultinomGrad(params, data_multinom$data, k, p)
    grad_constr_fn = function(par) {jacobian(constr_fn, par)}

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn, #objective function
      eval_grad_f = grad_obj_fn, #gradient of objective function
      eval_g_ineq = constr_fn, # constraint
      eval_jac_g_ineq = grad_constr_fn, #gradient of constraint
      opts = list("algorithm"="NLOPT_LD_MMA",
                  "xtol_rel"=1.0e-8,
                  "maxeval" = 1000)
                  )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                         nrow = k-1, 
                         ncol = p)
    
    #calculating MSE
    errorcoefs2 = (data_multinom$beta - optimized_coefs)^2
    errorintercept2 = (data_multinom$alpha - optimized_intercepts)^2
    errors = c(errorintercept2, errorcoefs2)
    df_MtoM = rbind(df_MtoM, errors)
  }
  
  return(df_MtoM)
}

Efficiency = c()
for (n in N) {
  print(n)
  test = simulate_efficiency(n, 3, 2, 10)
  Efficiency = rbind(Efficiency, colMeans(test))
}

#Show that this decreases as N -> infty
```

!!!!! 
TURNS OUT that the way you sample your coefficients can massively impact predictive performance, sampling from standard normal results in lower accuracy compared to sampling from from N(5,25) distribution, as its less likely that 1 class dominates.
!!!!!

```{r}
#predictive performance test: confusion matrices
N = 150
k = 3
p = 3
set.seed(seed)

beta_M = matrix(rnorm((k - 1) * p, mean = 2, sd = 2), ncol = p)
beta_signs = sample(c(-1, 1), p, replace = TRUE)
for (j in 1:p) {
  beta_M[, j] = beta_signs[j] * abs(beta_M[, j])
  }
alpha_M = rnorm(k - 1, mean = 2, sd = 2)
alpha_A = sort(rnorm(k - 1, mean = 2, sd = 2)) 
beta_A = rnorm(p, mean = 2, sd = 2)

Estimate_OM_class = function(N, k, p, alpha, beta, X){
  # N number of datapoints
  # k number of classes
  # p number of predictors
  # alpha intercepts
  # beta coefficient matrix

  # Compute linear predictors for each class (excluding reference class)
  eta = X %*% t(beta) + matrix(alpha, nrow = N, ncol = k - 1, byrow = TRUE) # N x (k-1)
  
  # Convert to probabilities using softmax transformation
  exp_eta = exp(cbind(0, eta)) # Add reference class with zero logit
  probs = exp_eta / rowSums(exp_eta) # Normalize
 
  #find which class has highest probability and return that class number
  classes = c()
  for (i in (1:N)){
    
    classes[i] = which.max(probs[i,]) - 1 #-1 for reference class
  }
  
  return(classes)
}

Estimate_AC_class = function(N, k, p, alpha, beta, X){
  # N number of datapoints
  # k number of classes
  # p number of predictors
  # alpha intercepts
  # beta coefficient matrix

  # Compute cumulative log-odds
  log_odds = matrix(NA, nrow = N, ncol = k - 1)
  for (j in 1:(k - 1)) {
    log_odds[, j] = alpha[j] + j * as.matrix(X) %*% beta
  }
  
  # Convert log odds to probabilities
  probs = exp(log_odds)
  probs = cbind(1, probs)  # Include baseline category (category 1)
  probs = probs / rowSums(probs)  # Normalize to sum to 1
  
  #find which class has highest probability and return that class number
  classes = c()
  for (i in (1:N)){
    classes[i] = which.max(probs[i,]) - 1 #-1 for reference class
  }
  
  return(classes)
}

Data_M = generate_ordinal_multinomial_data(N,k,p,alpha_M,beta_M)
Data_A = generate_adjacent_categories_data(N, k, p, alpha_A, beta_A)

###fitting multinom to multinom
    initial_params = c(rep(Data_M$alpha),  # Intercepts
                   as.vector(Data_M$beta))   # Coefficients
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    #wrapper functions
    obj_fn = function(params) LogLikMultinom(params, Data_M$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, Data_M$data, k, p)
    grad_obj_fn = function(params) LogLikMultinomGrad(params, Data_M$data, k, p)
    grad_constr_fn = function(par) {jacobian(constr_fn, par)}

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn, #objective function
      eval_grad_f = grad_obj_fn, #gradient of objective function
      eval_g_ineq = constr_fn, # constraint
      eval_jac_g_ineq = grad_constr_fn, #gradient of constraint
      opts = list("algorithm"="NLOPT_LD_MMA",
                  "xtol_rel"=1.0e-8,
                  "maxeval" = 10000)
                  )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                         nrow = k-1, 
                         ncol = p)

True_class = Data_M$data$Y
Pred_class = factor(Estimate_OM_class(N, k, p, optimized_intercepts, optimized_coefs, as.matrix(Data_M$data[,-1])), levels = 0:(k-1))
ConfMatrixMtoM = confusionMatrix(True_class, Pred_class)

###fitting AC to Multinom
    initial_params = c(rep(Data_M$alpha),  # Intercepts
                   as.vector(Data_M$beta))   # Coefficients
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    #wrapper functions
    obj_fn = function(params) LogLikMultinom(params, Data_A$data, k, p)
    constr_fn = function(params) Ineq_enforcer_multinom(params, Data_A$data, k, p)
    grad_obj_fn = function(params) LogLikMultinomGrad(params, Data_A$data, k, p)
    grad_constr_fn = function(par) {jacobian(constr_fn, par)}

    opt_result = nloptr(
      x0 = initial_params,
      eval_f = obj_fn, #objective function
      eval_grad_f = grad_obj_fn, #gradient of objective function
      eval_g_ineq = constr_fn, # constraint
      eval_jac_g_ineq = grad_constr_fn, #gradient of constraint
      opts = list("algorithm"="NLOPT_LD_MMA",
                  "xtol_rel"=1.0e-8,
                  "maxeval" = 10000)
                  )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$solution
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = matrix(optimized_params[k:length(optimized_params)], 
                         nrow = k-1, 
                         ncol = p)

True_class = Data_A$data$Y
Pred_class = factor(Estimate_OM_class(N, k, p, optimized_intercepts, optimized_coefs, as.matrix(Data_A$data[,-1])), levels = 0:(k-1))
ConfMatrixAtoM = confusionMatrix(True_class, Pred_class)
    
    
###fitting multinom to Adjacent
    initial_params = c(Data_A$alpha, as.vector(Data_A$beta))
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = LogLikAdjacent,           # Function to minimize
      gr = LogLikAdjacentGrad,       # gradient of Function
      data = Data_M$data,     # Pass the dataset separately
      k = k,
      p = p,
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = optimized_params[k:length(optimized_params)]
    
True_class = Data_M$data$Y
Pred_class = factor(Estimate_AC_class(N, k, p, optimized_intercepts, optimized_coefs, as.matrix(Data_M$data[,-1])), levels = 0:(k-1))
ConfMatrixMtoA = confusionMatrix(True_class, Pred_class)
    

    ###fitting Adjacent to Adjacent
    initial_params = c(Data_A$alpha, as.vector(Data_A$beta))
    initial_params = initial_params + rnorm(length(initial_params)) #add some random noise

    opt_result = optim(
      par = initial_params,          # Flattened initial parameters
      fn = LogLikAdjacent,           # Function to minimize
      gr = LogLikAdjacentGrad,       # gradient of Function
      data = Data_A$data,     # Pass the dataset separately
      k = k,
      p = p,
      method = "BFGS"                # Optimization method
      )

    #Reconstruction of original datashape and coefficients
    optimized_params = opt_result$par
    optimized_intercepts = optimized_params[1:(k-1)]
    optimized_coefs = optimized_params[k:length(optimized_params)]
    
True_class = Data_A$data$Y
Pred_class = factor(Estimate_AC_class(N, k, p, optimized_intercepts, optimized_coefs, as.matrix(Data_A$data[,-1])), levels = 0:(k-1))
ConfMatrixAtoA = confusionMatrix(True_class, Pred_class)
    

#prints of accuracy
ConfMatrixMtoM$overall
ConfMatrixMtoA$overall
ConfMatrixAtoM$overall
ConfMatrixAtoA$overall

```




